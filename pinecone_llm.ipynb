{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "chat  = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    model = 'gpt-3.5-turbo'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content= 'You are a helpful assistant.'),\n",
    "    HumanMessage(content='Hi AI, how are you today?'),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like some help to understand string theory.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! String theory is a theoretical framework in physics that attempts to describe the fundamental nature of the universe. It proposes that the fundamental building blocks of reality are not point-like particles, but tiny, one-dimensional \"strings\" that vibrate at different frequencies.\\n\\nHere are some key points to help you understand string theory:\\n\\n1. Strings: In string theory, the basic entities are tiny, vibrating strings. These strings can be open (with endpoints) or closed (forming a loop). The different vibrational patterns of these strings give rise to different particles and their properties.\\n\\n2. Extra dimensions: String theory requires the existence of more than the usual three dimensions of space and one dimension of time. It suggests that there may be additional, compactified dimensions beyond our perception.\\n\\n3. Unification: One of the main goals of string theory is to unify all the fundamental forces of nature, including gravity, electromagnetism, and the strong and weak nuclear forces. It provides a framework where all these forces can be described by the vibrations of the tiny strings.\\n\\n4. Quantum mechanics: String theory is a quantum theory, meaning it incorporates the principles of quantum mechanics. It describes particles and their interactions probabilistically, allowing for phenomena like particle-wave duality and quantum entanglement.\\n\\n5. Multiverse: String theory suggests the possibility of a \"multiverse,\" where our universe is just one of many possible universes, each with its own set of physical laws and properties. This idea arises from the many potential solutions and configurations in string theory.\\n\\nIt\\'s important to note that string theory is still a work in progress, and many aspects of it are still being researched and debated. It is a highly complex and mathematical theory that requires a deep understanding of physics and mathematics to fully grasp. Nonetheless, these key points should give you a general understanding of what string theory is about.\\n\\nIs there anything specific you would like to know or any other questions I can help with?')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! String theory is a theoretical framework in physics that attempts to describe the fundamental nature of the universe. It proposes that the fundamental building blocks of reality are not point-like particles, but tiny, one-dimensional \"strings\" that vibrate at different frequencies.\n",
      "\n",
      "Here are some key points to help you understand string theory:\n",
      "\n",
      "1. Strings: In string theory, the basic entities are tiny, vibrating strings. These strings can be open (with endpoints) or closed (forming a loop). The different vibrational patterns of these strings give rise to different particles and their properties.\n",
      "\n",
      "2. Extra dimensions: String theory requires the existence of more than the usual three dimensions of space and one dimension of time. It suggests that there may be additional, compactified dimensions beyond our perception.\n",
      "\n",
      "3. Unification: One of the main goals of string theory is to unify all the fundamental forces of nature, including gravity, electromagnetism, and the strong and weak nuclear forces. It provides a framework where all these forces can be described by the vibrations of the tiny strings.\n",
      "\n",
      "4. Quantum mechanics: String theory is a quantum theory, meaning it incorporates the principles of quantum mechanics. It describes particles and their interactions probabilistically, allowing for phenomena like particle-wave duality and quantum entanglement.\n",
      "\n",
      "5. Multiverse: String theory suggests the possibility of a \"multiverse,\" where our universe is just one of many possible universes, each with its own set of physical laws and properties. This idea arises from the many potential solutions and configurations in string theory.\n",
      "\n",
      "It's important to note that string theory is still a work in progress, and many aspects of it are still being researched and debated. It is a highly complex and mathematical theory that requires a deep understanding of physics and mathematics to fully grasp. Nonetheless, these key points should give you a general understanding of what string theory is about.\n",
      "\n",
      "Is there anything specific you would like to know or any other questions I can help with?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory has the potential to produce a unified theory because it provides a framework where all the fundamental forces of nature and their interactions can be described consistently. Here are a few reasons why string theory is seen as a candidate for unification:\n",
      "\n",
      "1. Consistency with quantum mechanics: String theory is a quantum theory, meaning it incorporates the principles of quantum mechanics. It allows for the probabilistic behavior of particles and the phenomena observed at the quantum level, such as particle-wave duality and quantum entanglement. This consistency with quantum mechanics is a significant advantage in seeking a unified theory.\n",
      "\n",
      "2. Gravitational force: One of the main challenges in physics is reconciling gravity, described by Einstein's general theory of relativity, with the other fundamental forces, described by quantum mechanics. String theory naturally includes gravity and treats it on par with the other forces. This suggests the possibility of a unified description of gravity and the other forces within the framework of string theory.\n",
      "\n",
      "3. Dualities and symmetries: String theory exhibits intriguing dualities and symmetries, which relate seemingly different theories to each other. These dualities imply that different formulations of string theory may be equivalent descriptions of the same physical reality. These symmetries and dualities provide hints that there may be a deeper underlying unity in the theory.\n",
      "\n",
      "4. Resolution of singularities: Singularities, such as those found in black holes or the Big Bang, are points where our current theories break down. String theory offers potential resolutions to these singularities, providing a more complete and consistent description of these extreme phenomena.\n",
      "\n",
      "5. Multiverse and the landscape: String theory suggests the possibility of a \"multiverse,\" where our universe is just one of many possible universes with different properties. This concept arises from the vast number of potential solutions and configurations in string theory, known as the \"landscape.\" The existence of a multiverse allows for the possibility of different laws of physics in different universes.\n",
      "\n",
      "It is important to note that while string theory shows promise for unification, it is still a highly speculative and mathematically challenging theory. The full understanding and empirical validation of string theory are ongoing areas of research and debate among physicists.\n",
      "\n",
      "I hope this helps clarify why physicists believe string theory has the potential to produce a unified theory. Let me know if you have any further questions!\n"
     ]
    }
   ],
   "source": [
    "# Add latest AI response to messages\n",
    "\n",
    "messages.append(res)\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content= 'Why do physicists believe it can produce a unified theory?'\n",
    ")\n",
    "\n",
    "# Add to messages \n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I couldn't find any specific information about the LLMCHAIN in LangChain. It's possible that LLMCHAIN is a term or concept specific to LangChain or a specific context that I'm not aware of. Without more information, it is difficult for me to provide a detailed explanation.\n",
      "\n",
      "If you can provide more context or details about what LLMCHAIN refers to within the LangChain framework, I may be able to assist you further.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content='Can you tell me something about the LLMCHAIN in LangChain?'\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.\",\n",
    "    \"An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me something about the LLMChain in LangChain in a Shakespearian way?\"\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "Query: {query}\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verily, the LLMChain in LangChain doth possess a grand purpose. 'Tis a chain of simple nature, yet it doth add a touch of functionality to the language models it encompasses. Throughout the realm of LangChain, it is widely employed in various chains and agents.\n",
      "\n",
      "An LLMChain is comprised of two noble components: a PromptTemplate and a language model, be it the esteemed LLM or a chat model. With great finesse, it doth format the prompt template using the input key values provided, and should memory key values be present, they too shall be utilized. Once the formatting is complete, the LLMChain doth pass the adorned string to the LLM, which in turn doth render its wisdom and return the LLM output.\n",
      "\n",
      "Thus, the LLMChain in LangChain doth serve as a bridge betwixt the prompt template and the language model, bringing forth a harmonious union of words and knowledge.\n",
      "\n",
      "Is there anything else I can assist thee with?\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLMChain in LangChain is a fundamental component that enhances the functionality of language models. It comprises a PromptTemplate and a language model, such as an LLM (Large Language Model) or a chat model. The PromptTemplate is formatted using input key values, and optionally memory key values if they are available. The formatted prompt is then passed to the language model, which generates a response based on the given input. The LLMChain plays a crucial role in processing and producing outputs in LangChain, and it is extensively utilized in other chains and agents within the system.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sabdo\\.conda\\envs\\patrun-llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 409/409 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/jamescalam--llama-2-arxiv-papers-chunked to C:/Users/sabdo/.cache/huggingface/datasets/jamescalam___json/jamescalam--llama-2-arxiv-papers-chunked-ea255a807f3039a6/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 14.4M/14.4M [00:01<00:00, 11.0MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:02<00:00,  2.68s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 44.28it/s]\n",
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/sabdo/.cache/huggingface/datasets/jamescalam___json/jamescalam--llama-2-arxiv-papers-chunked-ea255a807f3039a6/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=\"f5364a23-839d-4faf-9244-9183f68993a6\", \n",
    "              environment='us-east-1-aws')\n",
    "index = pinecone.Index('patrundev1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 5170}, 'default': {'vector_count': 200}},\n",
       " 'total_vector_count': 5370}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embed_model = OpenAIEmbeddings(model = 'text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating embeddings\n",
    "texts = [\n",
    "    'this is a chunk of text',\n",
    "    'then another chunk of text'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [02:43<00:00,  3.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 5170}, 'default': {'vector_count': 200}},\n",
       " 'total_vector_count': 5370}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sabdo\\.conda\\envs\\patrun-llm\\lib\\site-packages\\langchain_community\\vectorstores\\pinecone.py:62: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What's so special about Llama 2?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    results = vectorstore.similarity_search(query, k = 3)\n",
    "    source_knowledge = \"\\n\".join(x.page_content for x in results)\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query. \n",
    "\n",
    "    Contexts: \n",
    "    {source_knowledge}\n",
    "\n",
    "    Query:\n",
    "    {query} \"\"\"\n",
    "\n",
    "    return augmented_prompt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query. \n",
      "\n",
      "    Contexts: \n",
      "    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom\u0003\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
      "preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n",
      "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\n",
      "Grave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\n",
      "arXiv:2302.13971 , 2023.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "and Illia Polosukhin. Attention is all you need, 2017.\n",
      "Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\n",
      "David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\n",
      "multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint\n",
      "\n",
      "    Query:\n",
      "    What's so special about Llama 2? \n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. These LLMs, such as L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. They have been developed and released to outperform existing open-source chat models on various benchmarks, and based on humane evaluations for helpfulness and safety, they may serve as suitable substitutes for closed-source models. The Llama 2 models have undergone extensive fine-tuning and safety measures, aligning them with human preferences and enhancing usability and safety. This work represents a significant advancement in AI alignment research within the community.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the development of Llama 2, several safety measures were employed to enhance the safety of the fine-tuned language models (LLMs). These measures include safety-specific data annotation and tuning, red-teaming, iterative evaluations, and a focus on improving LLM safety.\n",
      "\n",
      "By using safety-specific data annotation and tuning, the developers aimed to ensure that the LLMs align with human preferences and enhance usability and safety. This involves carefully selecting and fine-tuning the models to make them more suitable for dialogue use cases.\n",
      "\n",
      "Red-teaming refers to the process of conducting adversarial testing to identify and address potential vulnerabilities and risks in the LLMs. This helps to improve the robustness and safety of the models by identifying and mitigating any potential issues.\n",
      "\n",
      "Iterative evaluations involve regularly assessing the LLMs' performance and safety through human evaluations. This allows for continuous improvement and refinement of the models based on feedback and observations from the evaluations.\n",
      "\n",
      "Overall, the developers of Llama 2 have taken these safety measures to ensure that the fine-tuned LLMs are as safe as possible, and they hope that the openness and transparency of their approach will enable the community to reproduce and further enhance the safety of these models.\n",
      "\n",
      "Please note that the specific details of the safety measures used may be further elaborated in the referenced research papers mentioned in the contexts.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"what safety measures were used in the development of llama 2?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patrun-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
